{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy \n",
    "import pandas as pd\n",
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from textblob import TextBlob\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import re\n",
    "import multiprocessing\n",
    "from time import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to fetch tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "consumer_key= 'Zm88MmZ4UYJ1o1SqZEUl5blzl'\n",
    "consumer_secret= '4u0TPzbqEoWt7xoH4mB0ttZyryPRZtKrSHqqvg4xG3r0PLVsm1'\n",
    "access_token= '1363507078325747721-TiUTfvsD0TfDXzwO0u8HqG0kcxQAEz'\n",
    "access_token_secret= 'rSGdxdY1d5LokxQUo4xmzvMGhOV8BslNiZGry9DanLpl3'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('farmersprotest.csv', 'a')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "q=''\n",
    "ids = set()\n",
    "for tweet in tweepy.Cursor(api.search, \n",
    "                    q=\"#FarmersProtest\", \n",
    "                    Since=\"2021-01-01\", \n",
    "                    tweet_mode='extended',\n",
    "                    #until=\"2014-02-15\", \n",
    "                    lang=\"en\").items(1000):\n",
    "    if (not tweet.retweeted) and ('RT @' not in tweet.full_text):\n",
    "     #Write a row to the csv file/ I use encode utf-8\n",
    "        csvWriter.writerow([tweet.created_at, tweet.full_text, tweet.favorite_count, tweet.retweet_count, tweet.id])\n",
    "     #print \"...%s tweets downloaded so far\" % (len(tweet.id))\n",
    "        ids.add(tweet.id) # add new id\n",
    "        print (\"number of unique ids seen so far: {}\",format(len(ids)))\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('farmersprotest_final.csv',header=None,names=['created_at'\\\n",
    "                                                                                             ,'text'\\\n",
    "                                                                                             ,'favorite_count'\\\n",
    "                                                                                            ,'retweet_count'\\\n",
    "                                                                                            ,'id'\\\n",
    "                                                                                            ,'screen_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the text to lowercase\n",
    "df['preprocessed']=df['text'].str.lower().str.replace('#farmersprotest',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove a given pattern\n",
    "def remove_pattern(input_text,pattern):\n",
    "    p=re.findall(pattern,input_text)\n",
    "    for i in p:\n",
    "        input_text=re.sub(i,\"\",input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed']=df['preprocessed'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))\n",
    "#Removing Usernames\n",
    "# clean_text=remove_pattern(remove_spaces,\"@[\\w]*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes punctuations,hyperlinks\n",
    "df['preprocessed']=df['preprocessed'].apply(lambda x: \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing white spaces\n",
    "df['preprocessed']=df['preprocessed'].apply(lambda x: re.sub('/s+',\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stopwords\n",
    "def removeStopWords(x):\n",
    "    words=nltk.tokenize.word_tokenize(x)\n",
    "#     words_new=[i for i in words if i not in stop+['r']]\n",
    "    words_new=[i for i in words if i not in ['r']]\n",
    "\n",
    "    return words_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing the most common hashtags\n",
    "df['preprocessed']=df['preprocessed'].str.replace('shaheedjawan',\"\")\n",
    "\n",
    "df['preprocessed']=df['preprocessed'].str.replace('shaheedkisan',\"\")\n",
    "\n",
    "df['preprocessed']=df['preprocessed'].str.replace('shaheed',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed']=df['preprocessed'].apply(lambda x: re.sub('/s+',\" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed']=df['preprocessed'].apply(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re,ove stop words\n",
    "df['token_words']=df['preprocessed'].apply(removeStopWords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization\n",
    "df['lemmatized_words']=df['token_words'].apply(lambda x:[ lemmatizer.lemmatize(text) for text in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_words']=df['lemmatized_words'].apply(lambda x:[i for i in x if(len(i) >1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_sentence']=df['lemmatized_words'].apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method1 -TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the polarity\n",
    "df['polarity']=df['lemmatized_sentence'].apply(lambda x:TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the subjectivity\n",
    "df['subjectivity']=df['lemmatized_sentence'].apply(lambda x:TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['subject_polar']=(df['subjectivity']+0.01)*df['polarity']# adding 0.01 in order to avoid subjectivity zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to assign classes\n",
    "def sentiment_classification(x):\n",
    "    if(x<0):\n",
    "        return 'Negative'\n",
    "    elif(x>0):\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_class']=df['polarity'].apply(sentiment_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_class_new']=df['subject_polar'].apply(sentiment_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result\n",
    "df['sentiment_class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_class_new'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Method 2 - Word2vec and K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row for row in df.lemmatized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_words']=df['lemmatized_words'].apply(lambda x:[i for i in x if(len(i) >1) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_count=Ignore all words and bigrams with total collected count lower than this value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases=Phrases(sent, min_count=2, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]#no difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases = Phrases(sent, min_count=1, progress_per=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Word 2 vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = Word2Vec(min_count=1,\n",
    "#                      seed=1,\n",
    "#                      workers=multiprocessing.cpu_count()-1)\n",
    "# w2v_model.build_vocab(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the model and building vocabulary\n",
    "start = time()\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=2,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     workers=multiprocessing.cpu_count()-1,\n",
    "                       seed=1\n",
    "                    ,sg=0)\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the word2vec model\n",
    "start = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "# w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors=w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(w2v_model.wv)\n",
    "vocab_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.save(\"word2vec.model\")\n",
    "# word_vectors = Word2Vec.load(\"word2vec.model\").wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_vectors=w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.key_to_index.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering mmodel\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3, max_iter=1000, random_state=True, n_init=100).fit(X=word_vectors.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "we are figuring out which cluster is having negative and positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_cluster_center = model.cluster_centers_[1]\n",
    "# negative_cluster_center = model.cluster_centers_[0]\n",
    "# neutral_cluster_center=model.cluster_centers_[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.key_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.cluster =words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assigning clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_value_predict(x):\n",
    "    if(x==0):\n",
    "        return -1\n",
    "    elif(x==1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value']=words['cluster'].apply(cluster_value_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the sentiment coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).mean()), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['closeness_score_min'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "# words['closeness_score_max'] = words.apply(lambda x: 1/(model.transform([x.vectors]).max()), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['sentiment_coeff_min'] = words.closeness_score_min * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words['sentiment_coeff_max'] = words.closeness_score_max * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words','cluster_value','sentiment_coeff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_dict = dict(zip(words['words'], words['sentiment_coeff']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm='l2')\n",
    "tfidf.fit(df.lemmatized_sentence)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(df.lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the TFIDF dictionary and replacing the tweets withtfidf scores\n",
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    " \n",
    "  \n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "   \n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.lemmatized_sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores =df.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
    "#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dictionary\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = df.lemmatized_sentence.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, df.id,df.lemmatized_sentence, df.text]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'id','lemmatized_sentence', 'tweets']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot products of 2 sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  converting bool (boolean to integer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "# replacement_df['sentiment'] = [1 if i==1 else 0 for i in replacement_df.sentiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacement_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the benchmark for each cluster\n",
    "replacement_df['prediction']=np.where(replacement_df['sentiment_rate']>1,'positive'\n",
    "                                      ,np.where(replacement_df['sentiment_rate']<-0.5,'negative','neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of the output\n",
    "replacement_df['prediction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scale the result\n",
    "def cluster_value_predict(x):\n",
    "    if(x>1):\n",
    "        return 1\n",
    "    elif(x<-0.4):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df['sentiment_class']=replacement_df['sentiment_rate'].apply(cluster_value_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df.to_csv('final_updated_out.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df.loc[11146]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* comparing textblob to user defined built alogirithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_algo=replacement_df['prediction'].value_counts().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_algo=df['sentiment_class_new'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "sns.set(color_codes=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textblob_algo['index']=textblob_algo['index'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combineOutput=defined_algo.merge(textblob_algo,on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combineOutput.rename(columns={'sentiment_class_new':'TextBlob','prediction':'Word2vec-Kmeans'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the visualisation\n",
    "combineOutput.plot(x=\"index\", y=[\"Word2vec-Kmeans\", \"TextBlob\"], kind=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combineOutput=defined_algo.merge(textblob_algo,on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({\n",
    "    'One': df1.set_index('Brand').Count, 'Two': df2.set_index('Brand').Count\n",
    "}, axis=1).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=defined_algo['index'],y=defined_algo['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
